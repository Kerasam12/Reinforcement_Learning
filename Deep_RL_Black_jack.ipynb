{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from copy import deepcopy, copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b19e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"Blackjack-v1\", natural = True , sab = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7199c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ACTION SPACE:\\n\",env.action_space, \": HIT or STICK\\n\")\n",
    "print(\"OBSERVATION SPACE:\\n\",env.observation_space, \": user_sum, dealer_card, if_usable_ace_of_user\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Action space is {} \".format(env.action_space))\n",
    "print(\"Observation space is {} \".format(env.observation_space))\n",
    "print(\"Reward range is {} \".format(env.reward_range))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066f53b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this cell fails you need to change the runtime of your colab notebook to GPU\n",
    "# Go to Runtime -> Change Runtime Type and select GPU\n",
    "assert torch.cuda.is_available(), \"GPU is not enabled\"\n",
    "\n",
    "#  Use gpu if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"tpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc5350",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, env, learning_rate=1e-3, device='cuda'):\n",
    "        super(DQN, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_inputs = env.observation_space.shape[0]\n",
    "        self.n_outputs = env.action_space.n\n",
    "        self.actions = np.arange(env.action_space.n)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        ### Construction of the neural network\n",
    "        self.model = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.n_inputs, 16, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 16, bias=True),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, self.n_outputs, bias=True))\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        ### Work with CUDA is allowed\n",
    "        if self.device == 'cuda':\n",
    "            self.model.cuda()\n",
    "            \n",
    "    \n",
    "    ### e-greedy method\n",
    "    def get_action(self, state, epsilon=0.05):\n",
    "        if np.random.random() < epsilon:\n",
    "            # random action\n",
    "            action = np.random.choice(self.actions)  \n",
    "        else:\n",
    "            # Q-value based action\n",
    "            qvals = self.get_qvals(state)  \n",
    "            action= torch.max(qvals, dim=-1)[1].item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def get_qvals(self, state):\n",
    "        if type(state) is tuple:\n",
    "            state = np.array([np.ravel(s) for s in state])\n",
    "        \n",
    "        state_t = torch.FloatTensor(state).to(device=self.device)\n",
    "        \n",
    "        return self.model(state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23722ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class experienceReplayBuffer:\n",
    "\n",
    "    def __init__(self, memory_size=50000, burn_in=10000):\n",
    "        self.memory_size = memory_size\n",
    "        self.burn_in = burn_in\n",
    "        self.buffer = namedtuple('Buffer', field_names=['state', 'action', 'reward', 'done', 'next_state'])\n",
    "        self.replay_memory = deque(maxlen=memory_size)\n",
    "\n",
    "        \n",
    "    ## We create a list of random indexes and package the experiences into Numpy arrays\n",
    "    # (makes it easier to calculate the loss later)\n",
    "    def sample_batch(self, batch_size=32):\n",
    "        samples = np.random.choice(len(self.replay_memory), batch_size, replace=False)\n",
    "        # Use asterisk operator to unpack deque \n",
    "        batch = zip(*[self.replay_memory[i] for i in samples])\n",
    "        \n",
    "        return batch\n",
    "\n",
    "    \n",
    "    ## New experiences are added \n",
    "    def append(self, state, action, reward, done, next_state):\n",
    "        self.replay_memory.append(self.buffer(state, action, reward, done, next_state))\n",
    "\n",
    "        \n",
    "    ## The buffer is filled with random experiences at the beginning of training\n",
    "    def burn_in_capacity(self):\n",
    "        return len(self.replay_memory) / self.burn_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e19fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, env, dnnetwork, buffer, epsilon=0.1, eps_decay=0.99, batch_size=32):\n",
    "        self.env = env\n",
    "        self.dnnetwork = dnnetwork\n",
    "        self.target_network = deepcopy(dnnetwork) # red objetivo (copia de la principal)\n",
    "        self.buffer = buffer\n",
    "        self.epsilon = epsilon\n",
    "        self.eps_decay = eps_decay\n",
    "        self.batch_size = batch_size\n",
    "        # block of the last X episodes to calculate the average reward \n",
    "        self.nblock = 100 \n",
    "        # average reward used to determine if the agent has learned to play\n",
    "        self.reward_threshold = self.env.spec.reward_threshold \n",
    "        \n",
    "        self.initialize()\n",
    "    \n",
    "    \n",
    "    def initialize(self):\n",
    "        self.update_loss = []\n",
    "        self.training_rewards = []\n",
    "        self.mean_training_rewards = []\n",
    "        self.sync_eps = []\n",
    "        self.total_reward = 0\n",
    "        self.step_count = 0\n",
    "        self.state0 = self.env.reset()[0]\n",
    "        \n",
    "    \n",
    "    ## Take new action\n",
    "    def take_step(self, eps, mode='train'):\n",
    "        if mode == 'explore': \n",
    "            # random action in burn-in and in the exploration phase (epsilon)\n",
    "            action = self.env.action_space.sample() \n",
    "        else:\n",
    "            # Action based on the Q-value (max Q-value)\n",
    "            action = self.dnnetwork.get_action(self.state0, eps)\n",
    "            self.step_count += 1\n",
    "            \n",
    "        # Execute action and get reward and new state\n",
    "        new_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        self.total_reward += reward\n",
    "        # save experience in the buffer\n",
    "        self.buffer.append(self.state0, action, reward, done, new_state) \n",
    "        self.state0 = new_state.copy()\n",
    "        \n",
    "        if done:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "        \n",
    "        return done\n",
    "\n",
    "    \n",
    "        \n",
    "    ## Training\n",
    "    def train(self, gamma=0.99, max_episodes=50000, \n",
    "              batch_size=32,\n",
    "              dnn_update_frequency=4,\n",
    "              dnn_sync_frequency=2000):\n",
    "        \n",
    "        self.gamma = gamma\n",
    "\n",
    "        # Fill the buffer with N random experiences\n",
    "        print(\"Filling replay buffer...\")\n",
    "        while self.buffer.burn_in_capacity() < 1:\n",
    "            self.take_step(self.epsilon, mode='explore')\n",
    "\n",
    "              \n",
    "        episode = 0\n",
    "        training = True\n",
    "        print(\"Training...\")\n",
    "        while training:\n",
    "            self.state0 = self.env.reset()[0]\n",
    "            self.total_reward = 0\n",
    "            gamedone = False\n",
    "            while gamedone == False:\n",
    "                # The agent takes an action\n",
    "                gamedone = self.take_step(self.epsilon, mode='train')\n",
    "               \n",
    "                # Upgrade main network\n",
    "                if self.step_count % dnn_update_frequency == 0:\n",
    "                    self.update()\n",
    "                # Synchronize the main network and the target network\n",
    "                if self.step_count % dnn_sync_frequency == 0:\n",
    "                    self.target_network.load_state_dict(\n",
    "                        self.dnnetwork.state_dict())\n",
    "                    self.sync_eps.append(episode)\n",
    "                    \n",
    "                if gamedone:                   \n",
    "                    episode += 1\n",
    "                    # Save the rewards\n",
    "                    self.training_rewards.append(self.total_reward) \n",
    "                    self.update_loss = []\n",
    "                    # Calculate the average reward for the last X episodes\n",
    "                    mean_rewards = np.mean(self.training_rewards[-self.nblock:])\n",
    "                    self.mean_training_rewards.append(mean_rewards)\n",
    "\n",
    "                    print(\"\\rEpisode {:d} Mean Rewards {:.2f} Epsilon {}\\t\\t\".format(episode, mean_rewards, self.epsilon), end=\"\")\n",
    "                    \n",
    "                    # Check if there are still episodes left\n",
    "                    if episode >= max_episodes:\n",
    "                        training = False\n",
    "                        print('\\nEpisode limit reached.')\n",
    "                        break\n",
    "                    \n",
    "                    # The game ends if the average reward has reached the threshold\n",
    "                    if mean_rewards >= self.reward_threshold:\n",
    "                        training = False\n",
    "                        print('\\nEnvironment solved in {} episodes!'.format(episode))\n",
    "                        break\n",
    "                    \n",
    "                    # Update epsilon according to the fixed decay rate\n",
    "                    self.epsilon = max(self.epsilon * self.eps_decay, 0.01)\n",
    "                    \n",
    "                \n",
    "    ## Loss calculation           \n",
    "    def calculate_loss(self, batch):\n",
    "        # Separate the variables of the experience and convert them to tensors\n",
    "        states, actions, rewards, dones, next_states = [i for i in batch] \n",
    "        rewards_vals = torch.FloatTensor(rewards).to(device=self.dnnetwork.device) \n",
    "        actions_vals = torch.LongTensor(np.array(actions)).reshape(-1,1).to(device=self.dnnetwork.device)\n",
    "        dones_t = torch.BoolTensor(dones).to(device=self.dnnetwork.device)\n",
    "        \n",
    "        # Obtain the Q values of the main network\n",
    "        qvals = torch.gather(self.dnnetwork.get_qvals(states), 1, actions_vals)\n",
    "        \n",
    "        # Obtain the target Q values.\n",
    "        # The detach() parameter prevents these values from updating the target network\n",
    "        qvals_next = torch.max(self.target_network.get_qvals(next_states), dim=-1)[0].detach()\n",
    "        # 0 in terminal states\n",
    "        qvals_next[dones_t] = 0 \n",
    "        \n",
    "        # Calculate the Bellman equation\n",
    "        expected_qvals = self.gamma * qvals_next + rewards_vals\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = torch.nn.MSELoss()(qvals, expected_qvals.reshape(-1,1))\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def update(self):\n",
    "        # Remove any gradient\n",
    "        self.dnnetwork.optimizer.zero_grad()  \n",
    "        # Select a subset from the buffer\n",
    "        batch = self.buffer.sample_batch(batch_size=self.batch_size) \n",
    "        # Calculate the loss\n",
    "        loss = self.calculate_loss(batch) \n",
    "        # Difference to get the gradients\n",
    "        loss.backward() \n",
    "        # Apply the gradients to the neural network\n",
    "        self.dnnetwork.optimizer.step() \n",
    "        # Save loss values\n",
    "        if self.dnnetwork.device == 'cuda':\n",
    "            self.update_loss.append(loss.detach().cpu().numpy())\n",
    "        else:\n",
    "            self.update_loss.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb41e3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch12",
   "language": "python",
   "name": "pytorch12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c80501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043b72c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Blackjack-v1\", natural = True , sab = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1000675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f312ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 9, 0), -1.0, True, False, {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n",
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4865733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30, 9, 0), -1.0, True, False, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4135317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTION SPACE:\n",
      " Discrete(2) : HIT or STICK\n",
      "\n",
      "OBSERVATION SPACE:\n",
      " Tuple(Discrete(32), Discrete(11), Discrete(2)) : user_sum, dealer_card, if_usable_ace_of_user\n"
     ]
    }
   ],
   "source": [
    "print(\"ACTION SPACE:\\n\",env.action_space, \": HIT or STICK\\n\")\n",
    "print(\"OBSERVATION SPACE:\\n\",env.observation_space, \": user_sum, dealer_card, if_usable_ace_of_user\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff65ece",
   "metadata": {},
   "source": [
    "## 2.1 Naı̈ve Policy\n",
    "Implement an agent that carries out the following deterministic policy:\n",
    "\n",
    "• The agent will stick if it gets a score of 20 or 21.\n",
    "\n",
    "• Otherwise, it will hit.\n",
    "\n",
    "Questions:\n",
    "1. Using this agent, simulate 100,000 games and calculate the agent’s return\n",
    "(total accumulated reward).\n",
    "2. Additionally, calculate the % of wins, natural wins, losses and draws.\n",
    "3. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0413335e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveAgent():\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        \n",
    "        \n",
    "    def apply_policy(self,state):\n",
    "        \n",
    "        if state[0] >= 20:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "            \n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e738ac7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: -53936.0\n",
      "num_wins: 21.046 %\n",
      "num_natural_wins: 0.0 %\n",
      "num loses: 74.982 %\n",
      "num_draws: 3.972 %\n",
      "total_played: 100000\n"
     ]
    }
   ],
   "source": [
    "num_plays = 100000\n",
    "naive_agent = NaiveAgent(env)\n",
    "total_reward = 0 \n",
    "wins = 0\n",
    "natural_wins = 0\n",
    "draws = 0 \n",
    "loses = 0\n",
    "\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "for ply in range(num_plays):\n",
    "    \n",
    "    env.reset()\n",
    "    action = 1 \n",
    "    \n",
    "    while not terminated or truncated:\n",
    "        new_state, reward, terminated, truncated,_ = env.step(action)\n",
    "        \n",
    "        total_reward += reward\n",
    "        action = naive_agent.apply_policy(new_state)\n",
    "        \n",
    "        if reward == 1:\n",
    "            wins += 1\n",
    "        elif reward == 1.5:\n",
    "            natural_wins += 1\n",
    "        elif reward == -1:\n",
    "            loses += 1\n",
    "        elif reward == 0 and (terminated or truncated):\n",
    "            draws += 1 \n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "print(\"total_reward:\", total_reward)\n",
    "print(\"num_wins:\", (wins/num_plays)*100,\"%\")\n",
    "print(\"num_natural_wins:\",(natural_wins/num_plays)*100,\"%\")\n",
    "print(\"num loses:\", (loses/num_plays)*100,\"%\")\n",
    "print(\"num_draws:\", (draws/num_plays)*100,\"%\")\n",
    "print(\"total_played:\", wins+loses+natural_wins + draws)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ecb560",
   "metadata": {},
   "source": [
    "### Comment of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef059f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79774ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc43a118",
   "metadata": {},
   "source": [
    "## 2.2 Monte Carlo method\n",
    "The objective of this section is to estimate the optimal policy using Monte Carlo\n",
    "methods, specifically we will study algorithm “Control using MC methods with\n",
    "initial explorations”.\n",
    "Questions:\n",
    "\n",
    "1. Implement Algorithm 1 using the following parameters:\n",
    "\n",
    "• Number of episodes = 5,000,000\n",
    "\n",
    "• Discount factor = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe09adf",
   "metadata": {},
   "source": [
    "2. Implement a function that prints on the screen the optimal policy found for\n",
    "each state (similar to figure 2).\n",
    "3. Using the agent obtained through the Monte Carlo method, simulate 100,000\n",
    "games and calculate the agent’s return (total accumulated reward).\n",
    "4. Additionally, calculate the % of wins, natural wins, losses and draws.\n",
    "5. Compare the optimal policy found by our agent to the optimal policy (see\n",
    "figure 2).\n",
    "6. Comment on the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd766c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbc50182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Agent():\n",
    "    def __init__(self,env):\n",
    "        self.env = env\n",
    "        self.Returns = collections.defaultdict(list)\n",
    "        self.possible_user_sum = self.env.observation_space[0].n\n",
    "        self.possible_dealer_sum = self.env.observation_space[1].n\n",
    "        self.usable_ace = self.env.observation_space[2].n\n",
    "        self.possible_actions = self.env.action_space.n\n",
    "        self.Q = np.zeros((self.possible_user_sum, self.possible_dealer_sum, self.usable_ace, self.possible_actions))\n",
    "        self.policy = np.ones((self.possible_user_sum, self.possible_dealer_sum, self.usable_ace, self.possible_actions))/2\n",
    "        \n",
    "    \n",
    "    def update_Returns(self,state, action,G):\n",
    "        \n",
    "        self.Returns[(state,action)].append(G)\n",
    "        \n",
    "    def update_Q(self,state, action):\n",
    "        self.Q[state[0],state[1],state[2], action] = np.mean(self.Returns[(state,action)])\n",
    "        \n",
    "    def epsilon_greedy_policy(self,state,epsilon):\n",
    "        '''total_probs = np.ones((self.possible_user_sum, self.possible_dealer_sum, self.usable_ace, self.possible_actions))/2\n",
    "        for pu in range(self.possible_user_sum):\n",
    "            for pd in range(self.possible_dealer_sum):\n",
    "                for a in range(self.usable_ace):'''\n",
    "        probs = (np.ones(self.possible_actions) * epsilon) / self.possible_actions\n",
    "        best_action = np.argmax(self.Q[state[0]][state[1]][state[2]])\n",
    "        probs[best_action] += 1.0 - epsilon\n",
    "        #total_probs[pu][pd][a] = probs\n",
    "        self.policy[state[0],state[1],state[2]] = probs\n",
    "                    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7b78e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env,agent):\n",
    "    obs,_ = env.reset()\n",
    "    action =  np.random.choice(range(len(agent.policy[obs[0],obs[1],obs[2]])), p = agent.policy[obs[0],obs[1],obs[2]])\n",
    "    EPS = []\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "    while not terminated or truncated:\n",
    "        \n",
    "        new_state, reward, terminated, truncated,_ = env.step(action)\n",
    "        \n",
    "        EPS.append((obs,action,reward))\n",
    "        total_reward += reward\n",
    "        #print(agent.policy[new_state[0],new_state[1],new_state[2]])\n",
    "        action = np.random.choice(range(len(agent.policy[new_state[0],new_state[1],new_state[2]])), p = agent.policy[new_state[0],new_state[1],new_state[2]])\n",
    "        obs = new_state\n",
    "    return EPS\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31833456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15, 8, 0), {})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4a69836",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 5000000\n",
    "discount_factor = 1 \n",
    "epsilon = 0.1\n",
    "epsilon_decay = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d2028f",
   "metadata": {},
   "source": [
    "Define the general loop for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd54e84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n",
      "900000\n",
      "1000000\n",
      "1100000\n",
      "1200000\n",
      "1300000\n",
      "1400000\n",
      "1500000\n",
      "1600000\n",
      "1700000\n",
      "1800000\n",
      "1900000\n",
      "2000000\n",
      "2100000\n",
      "2200000\n",
      "2300000\n",
      "2400000\n",
      "2500000\n",
      "2600000\n",
      "2700000\n",
      "2800000\n",
      "2900000\n",
      "3000000\n",
      "3100000\n",
      "3200000\n",
      "3300000\n",
      "3400000\n",
      "3500000\n",
      "3600000\n",
      "3700000\n",
      "3800000\n",
      "3900000\n",
      "4000000\n",
      "4100000\n",
      "4200000\n",
      "4300000\n",
      "4400000\n",
      "4500000\n",
      "4600000\n",
      "4700000\n",
      "4800000\n",
      "4900000\n"
     ]
    }
   ],
   "source": [
    "mc_agent = MC_Agent(env)\n",
    "for i in range(num_episodes):\n",
    "    \n",
    "    EPS = generate_episode(env,mc_agent)\n",
    "    G = 0 \n",
    "    seen_states = []\n",
    "    for eps in reversed(EPS):\n",
    "        G = (discount_factor*G) + eps[2]\n",
    "        if (eps[0], eps[1]) not in seen_states:\n",
    "            seen_states.append((eps[0], eps[1]))\n",
    "            mc_agent.update_Returns(eps[0],eps[1],G)\n",
    "            mc_agent.update_Q(eps[0],eps[1])\n",
    "            mc_agent.epsilon_greedy_policy(eps[0],epsilon)\n",
    "    if i% 1000 == 0:\n",
    "        pass\n",
    "        #epsilon = epsilon *epsilon_decay \n",
    "    if i% 100000 == 0:\n",
    "        print(i)\n",
    "        #print(mc_agent.policy)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "377fe4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env,agent):\n",
    "    num_plays = 100000\n",
    "\n",
    "    total_reward = 0 \n",
    "    wins = 0\n",
    "    natural_wins = 0\n",
    "    draws = 0 \n",
    "    loses = 0\n",
    "    \n",
    "\n",
    "    terminated = False\n",
    "    truncated = False\n",
    "\n",
    "    for ply in range(num_plays):\n",
    "\n",
    "        state,_ = env.reset()\n",
    "        action = np.argmax(agent.policy[state[0],state[1],state[2]]) \n",
    "\n",
    "        while not terminated or truncated:\n",
    "            new_state, reward, terminated, truncated,_ = env.step(action)\n",
    "\n",
    "            total_reward += reward\n",
    "            action = np.argmax(agent.Q[new_state[0],new_state[1],new_state[2]])\n",
    "\n",
    "        if reward == 1:\n",
    "            wins += 1\n",
    "        elif reward == 1.5:\n",
    "            natural_wins += 1\n",
    "        elif reward == -1:\n",
    "            loses += 1\n",
    "        elif reward == 0 and (terminated or truncated):\n",
    "            draws += 1 \n",
    "        \n",
    "                \n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "    print(\"total_reward:\", total_reward)\n",
    "    print(\"num_wins:\", (wins/num_plays)*100,\"%\")\n",
    "    print(\"num_natural_wins:\",(natural_wins/num_plays)*100,\"%\")\n",
    "    print(\"num loses:\", (loses/num_plays)*100,\"%\")\n",
    "    print(\"num_draws:\", (draws/num_plays)*100,\"%\")\n",
    "    \n",
    "    print(\"total_played:\", wins+loses+natural_wins + draws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47ffb511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]],\n",
       "\n",
       "\n",
       "       [[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]],\n",
       "\n",
       "\n",
       "       [[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]],\n",
       "\n",
       "\n",
       "       [[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]],\n",
       "\n",
       "\n",
       "       [[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tempfile import TemporaryFile\n",
    "\n",
    "outfile = TemporaryFile()\n",
    "np.save(outfile, mc_agent.policy)\n",
    "_ = outfile.seek(0) # Only needed here to simulate closing & reopening file\n",
    "\n",
    "np.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3e9cda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]],\n",
       "\n",
       "\n",
       "       [[[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.]]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_agent.Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da3ffb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('mc_policy.npy', mc_agent.policy)\n",
    "loaded_array = np.load('mc_policy.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ece56525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]],\n",
       "\n",
       "\n",
       "       [[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]],\n",
       "\n",
       "\n",
       "       [[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]],\n",
       "\n",
       "\n",
       "       [[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]],\n",
       "\n",
       "\n",
       "       [[[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]],\n",
       "\n",
       "        [[0.5, 0.5],\n",
       "         [0.5, 0.5]]]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59add45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_reward: -3623.0\n",
      "num_wins: 38.23 %\n",
      "num_natural_wins: 4.24 %\n",
      "num loses: 48.213 %\n",
      "num_draws: 9.317 %\n",
      "total_played: 100000\n"
     ]
    }
   ],
   "source": [
    "run_episode(env,mc_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d607d0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "pol = mc_agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6439b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_use_ace= np.zeros((pol.shape[0],pol.shape[1]))\n",
    "not_use_ace_deal = np.zeros((pol.shape[0],pol.shape[1]))\n",
    "\n",
    "usable_ace= np.zeros((pol.shape[0],pol.shape[1]))\n",
    "usable_ace_deal = []\n",
    "pol = mc_agent.policy\n",
    "for usr_sum in range(pol.shape[0]):\n",
    "    for deal_sum in range(pol.shape[1]):\n",
    "        for a in range(pol.shape[2]):\n",
    "            if a == 0:\n",
    "                #print(np.argmax(pol[usr_sum,deal_sum,a]))\n",
    "                not_use_ace[usr_sum][deal_sum] = np.argmax(pol[usr_sum,deal_sum,a])\n",
    "                #not_use_ace.append(np.max(pol[usr_sum,deal_sum,a]))\n",
    "            elif a == 1:\n",
    "                usable_ace[usr_sum][deal_sum] = np.argmax(pol[usr_sum,deal_sum,a])\n",
    "                #usable_ace.append(np.max(pol[usr_sum,deal_sum,a]))\n",
    "                \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8d2c65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.],\n",
       "       [0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usable_ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81b2dad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.],\n",
       "       [0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1.],\n",
       "       [0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1.],\n",
       "       [0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "       [0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_use_ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad2a9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib.colors import BoundaryNorm\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from numpy import meshgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fe1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_use_ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df1b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.max(mc_agent.policy).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863a7898",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_agent.policy[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b5a619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x7f95d0944d90>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Z = not_use_ace\n",
    "x = np.arange(pol.shape[0])  # \n",
    "print(x)\n",
    "y = np.arange(pol.shape[1])  # \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.pcolormesh(y, x, Z, vmin=np.min(Z), vmax=np.max(Z), shading = \"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60e0cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51203365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d4420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "def plot_policy(policy):\n",
    "\n",
    "    def get_Z(player_hand, dealer_showing, usable_ace):\n",
    "        if policy[player_hand, dealer_showing, usable_ace] != [0.5,0.5]:\n",
    "            return policy[player_hand, dealer_showing, usable_ace]\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def get_figure(usable_ace, ax):\n",
    "        x_range = np.arange(1, 11)\n",
    "        y_range = np.arange(11, 22)\n",
    "        X, Y = np.meshgrid(x_range, y_range)\n",
    "        Z = np.array([[get_Z(player_hand, dealer_showing, usable_ace) for dealer_showing in x_range] for player_hand in range(21, 10, -1)])\n",
    "        surf = ax.imshow(Z, cmap=plt.get_cmap('Accent', 2), vmin=0, vmax=1, extent=[0.5, 10.5, 10.5, 21.5])\n",
    "        plt.xticks(x_range, ('A', '2', '3', '4', '5', '6', '7', '8', '9', '10'))\n",
    "        plt.yticks(y_range)\n",
    "        ax.set_xlabel('Dealer Showing')\n",
    "        ax.set_ylabel('Player Hand')\n",
    "        ax.grid(color='black', linestyle='-', linewidth=1)\n",
    "        divider = make_axes_locatable(ax)\n",
    "        cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "        cbar = plt.colorbar(surf, ticks=[0, 1], cax=cax)\n",
    "        cbar.ax.set_yticklabels(['0 (STICK)','1 (HIT)'])\n",
    "        cbar.ax.invert_yaxis() \n",
    "            \n",
    "    fig = plt.figure(figsize=(12, 12))\n",
    "    ax = fig.add_subplot(121)\n",
    "    ax.set_title('Usable Ace', fontsize=16)\n",
    "    get_figure(True, ax)\n",
    "    ax = fig.add_subplot(122)\n",
    "    ax.set_title('No Usable Ace', fontsize=16)\n",
    "    get_figure(False, ax)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a398eec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8764b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_policy(mc_agent.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a69923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7635f16a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d114db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
